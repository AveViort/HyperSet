<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: fpc package overview</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for fpc-package {fpc}"><tr><td>fpc-package {fpc}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>fpc package overview</h2>

<h3>Description</h3>

<p>Here is a list of the main functions in package fpc. Most other
functions are auxiliary functions for these.
</p>


<h3>Clustering methods</h3>


<dl>
<dt>dbscan</dt><dd><p>Computes DBSCAN density based clustering as introduced
in Ester et al. (1996).</p>
</dd>
<dt>fixmahal</dt><dd><p>Mahalanobis Fixed Point Clustering, Hennig and
Christlieb (2002), Hennig (2005).</p>
</dd>
<dt>fixreg</dt><dd><p>Regression Fixed Point Clustering, Hennig (2003).</p>
</dd>
<dt>flexmixedruns</dt><dd><p>This fits a latent class model to
data with mixed type continuous/nominal variables. Actually it
calls a method for <code><a href="../../flexmix/html/flexmix.html">flexmix</a></code>.</p>
</dd>
<dt>mergenormals</dt><dd><p>Clustering by merging components of a Gaussian
mixture, see Hennig (2010).</p>
</dd>
<dt>regmix</dt><dd><p>ML-fit of a mixture of linear regression models, see
DeSarbo and Cron (1988).</p>
</dd>
</dl>


<h3>Cluster validity indexes and estimation of the number of clusters</h3>


<dl>
<dt>cluster.stats</dt><dd><p>This computes several cluster validity
statistics from a clustering and a dissimilarity matrix including
the Calinski-Harabasz index, the adjusted Rand index and other
statistics explained in Gordon (1999) as well as several
characterising
measures such as average between cluster and within cluster
dissimilarity and separation. See also <code><a href="calinhara.html">calinhara</a></code>,
<code><a href="dudahart2.html">dudahart2</a></code> for specific indexes.</p>
</dd>
<dt>prediction.strength</dt><dd><p>Estimates the number of clusters by
computing the prediction strength of a
clustering of a dataset into different numbers of components for
various clustering methods, see
Tibshirani and Walther (2005). In fact, this is more flexible than
what is in the original paper, because it can use
point classification schemes that work better with clustering
methods other than k-means.</p>
</dd>
<dt>nselectboot</dt><dd><p>Estimates the number of clusters by bootstrap
stability selection, see Fang and Wang (2012). This is quite
flexible regarding clustering methods and point classification
schemes and also allows for dissimilarity data.</p>
</dd> 
</dl>


<h3>Cluster visualisation and validation</h3>


<dl>
<dt>clucols</dt><dd><p>Sets of colours and symbols useful for cluster plotting.</p>
</dd>
<dt>clusterboot</dt><dd><p>Cluster-wise stability assessment of a
clustering. Clusterings are performed on resampled data to see for
every cluster of the original dataset how well this is
reproduced. See Hennig (2007) for details.</p>
</dd>
<dt>cluster.varstats</dt><dd><p>Extracts variable-wise information for every
cluster in order to help with cluster interpretation.</p>
</dd>
<dt>plotcluster</dt><dd><p>Visualisation of a clustering or grouping in data
by various linear projection methods that optimise the separation
between clusters, or between a single cluster and the rest of the
data according to Hennig (2004) including classical methods such
as discriminant coordinates. This calls the function
<code><a href="discrproj.html">discrproj</a></code>, which is a bit more flexible but doesn't
produce a plot itself.</p>
</dd>
<dt>ridgeline.diagnosis</dt><dd><p>Plots and diagnostics for assessing
modality of Gaussian mixtures, see Ray and Lindsay (2005).</p>
</dd>
<dt>weightplots</dt><dd><p>Plots to diagnose component separation in
Gaussian mixtures, see Hennig (2010).</p>
</dd>
<dt>localshape</dt><dd><p>Local shape matrix, can be used for finding
clusters in connection with function <code>ics</code> in package
<code>ICS</code>, see Hennig's
discussion and rejoinder of Tyler et al. (2009).</p>
</dd>
</dl>


<h3>Useful wrapper functions for clustering methods</h3>


<dl>
<dt>kmeansCBI</dt><dd><p>This and other &quot;CBI&quot;-functions (see the
<code><a href="kmeansCBI.html">kmeansCBI</a></code>-help page) are unified wrappers for
various clustering methods in R that may be useful because they do
in one step for what you normally may need to do a bit more in R
(for example fitting a Gaussian mixture with noise component in
package mclust).</p>
</dd>
<dt>kmeansruns</dt><dd><p>This calls <code><a href="../../stats/html/kmeans.html">kmeans</a></code> for the k-means
clustering method and includes estimation of the number of
clusters and finding an optimal solution from several starting
points.</p>
</dd>
<dt>pamk</dt><dd><p>This calls <code><a href="../../cluster/html/pam.html">pam</a></code> and
<code><a href="../../cluster/html/clara.html">clara</a></code> for the partitioning around medoids 
clustering method (Kaufman and Rouseeuw, 1990) and includes two
different ways of estimating the number of clusters.</p>
</dd>
</dl>


<h3>Author(s)</h3>

<p>Christian Hennig
<a href="mailto:c.hennig@ucl.ac.uk">c.hennig@ucl.ac.uk</a>
<a href="http://www.homepages.ucl.ac.uk/~ucakche/">http://www.homepages.ucl.ac.uk/~ucakche/</a>
</p>


<h3>References</h3>

<p>DeSarbo, W. S. and Cron, W. L. (1988) A maximum likelihood methodology
for clusterwise linear regression, <em>Journal of
Classification</em> 5, 249-282.
</p>
<p>Ester, M., Kriegel, H.-P., Sander, J. and Xu, X.
(1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial
Databases with Noise.
<em>Proceedings of 2nd International Conference on
Knowledge Discovery and Data
Mining (KDD-96).</em>
</p>
<p>Fang, Y. and Wang, J. (2012) Selection of the number of clusters via
the bootstrap method. <em>Computational Statistics and Data
Analysis</em>, 56, 468-477.
</p>
<p>Gordon, A. D. (1999) <em>Classification</em>, 2nd ed. Chapman and Hall.
</p>
<p>Hennig, C. (2003) Clusters, outliers and regression:
fixed point clusters, <em>Journal of
Multivariate Analysis</em> 86, 183-212.
</p>
<p>Hennig, C. (2004) Asymmetric linear dimension reduction for classification.
<em>Journal of Computational and Graphical Statistics</em>, 13, 930-945 .
</p>
<p>Hennig, C. (2005) Fuzzy and Crisp Mahalanobis Fixed Point Clusters,
in Baier, D., Decker, R., and Schmidt-Thieme, L. (eds.):
<em>Data Analysis and Decision Support</em>. Springer, Heidelberg,
47-56, <a href="http://www.homepages.ucl.ac.uk/~ucakche/papers/fuzzyfix.ps">http://www.homepages.ucl.ac.uk/~ucakche/papers/fuzzyfix.ps</a>
</p>
<p>Hennig, C. (2007) Cluster-wise assessment of cluster
stability. <em>Computational Statistics and Data Analysis</em>,
52, 258-271.
</p>
<p>Hennig, C. (2010) Methods for merging Gaussian mixture components,
<em>Advances in Data Analysis and Classification</em>, 4, 3-34.
</p>
<p>Hennig, C. and Christlieb, N. (2002) Validating visual clusters in
large datasets: Fixed point clusters of spectral features,
<em>Computational Statistics and Data Analysis</em> 40, 723-739.
</p>
<p>Kaufman, L. and Rousseeuw, P.J. (1990). &quot;Finding Groups in Data:
An Introduction to Cluster Analysis&quot;. Wiley, New York.
</p>
<p>Ray, S. and Lindsay, B. G. (2005) The Topography of Multivariate 
Normal Mixtures, <em>Annals of Statistics</em>, 33, 2042-2065.
</p>
<p>Tibshirani, R. and Walther, G. (2005) Cluster Validation by 
Prediction Strength, <em>Journal of Computational and Graphical 
Statistics</em>, 14, 511-528.
</p>

<hr /><div style="text-align: center;">[Package <em>fpc</em> version 2.1-11.1 <a href="00Index.html">Index</a>]</div>
</body></html>
